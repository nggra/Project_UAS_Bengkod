# -*- coding: utf-8 -*-
"""PROJECT_UAS_BENGKOD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qvuxgGLBfmpSD8XMlTHHL2rRHFjJS4gl
"""

#!pip install streamlit

import streamlit as st
import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#!pip install imblearn



from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

#import joblib
import warnings
warnings.filterwarnings('ignore')

file_path = 'ObesityDataSet.csv'
df = pd.read_csv(file_path)

head = df.head()
info = df.info()
description = df.describe(include='all')

head, description

print(df.info())

# Konversi kolom numerik ke float
numeric_columns = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']
for col in numeric_columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')  # ubah ke float, NaN jika gagal

# Visualisasi distribusi label
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='NObeyesdad', order=df['NObeyesdad'].value_counts().index)
plt.xticks(rotation=45)
plt.title('Distribusi Kelas NObeyesdad')
plt.tight_layout()
plt.show()

# Cek missing values
missing_values = df.isnull().sum()
print(df.isnull().sum())

# Cek data duplikat
duplicates = df.duplicated().sum()
print(df.duplicated().sum())

# Cek nilai unik tiap kolom
unique_values = df.nunique()
print(df.nunique())

# Boxplot untuk mendeteksi outlier pada kolom numerik
plt.figure(figsize=(14, 10))
for i, col in enumerate(numeric_columns, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(x=df[col])
    plt.title(f'Boxplot of {col}')
plt.tight_layout()
plt.show()

missing_values, duplicates, unique_values

# Salin data asli
df_clean = df.copy()

print("Missing values:\n", df_clean.isnull().sum())

# Hapus data duplikat
df_clean = df_clean.drop_duplicates()

# Kategorikal: imputasi dengan modus
cat_columns = df_clean.select_dtypes(include='object').columns.drop('NObeyesdad')
cat_imputer = SimpleImputer(strategy='most_frequent')
df_clean[cat_columns] = cat_imputer.fit_transform(df_clean[cat_columns])

print("Menghapus outlier menggunakan metode IQR...")

outlier_counts = {}

for col in numeric_columns:
    Q1 = df_clean[col].quantile(0.25)
    Q3 = df_clean[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    before_rows = df_clean.shape[0]
    df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]
    after_rows = df_clean.shape[0]
    outlier_counts[col] = before_rows - after_rows

# Tampilkan hasil penghapusan per kolom
for col, count in outlier_counts.items():
    print(f"- {col}: {count} baris dihapus")

#Tentukan kolom kategorikal dan numerik
cat_columns = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']
numeric_columns = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']

#Encoding kategorikal
from sklearn.preprocessing import LabelEncoder, StandardScaler
encoder = LabelEncoder()
for col in cat_columns:
    df_clean[col] = encoder.fit_transform(df_clean[col])

# Encode target (wajib setelah encode fitur)
df_clean['NObeyesdad'] = encoder.fit_transform(df_clean['NObeyesdad'])

# Normalisasi/standarisasi data numerik
scaler = StandardScaler()
df_clean[numeric_columns] = scaler.fit_transform(df_clean[numeric_columns])

# Save the scaler
joblib.dump(scaler, "scaler.pkl")
joblib.dump(encoder, "label_encoder.pkl")
#joblib.dump(best_rf, "random_forest_model.pkl")



# Pisahkan fitur dan target
X = df_clean.drop('NObeyesdad', axis=1)
y = df_clean['NObeyesdad']

# Tampilkan distribusi kelas
print("Distribusi sebelum SMOTE:")
print(y.value_counts())

# Hitung korelasi numerik
corr_matrix = df.corr(numeric_only=True)
plt.figure(figsize=(10, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

# Cek ulang info dataset setelah preprocessing
df_clean.info()



for col in numeric_columns:
    plt.figure(figsize=(5, 3))
    sns.boxplot(x=df_clean[col])
    plt.title(f'Boxplot {col} (Setelah Outlier Dihapus)')
    plt.tight_layout()
    plt.show()

# Kesimpulan keseluruhan tahap EDA
print("\nKESIMPULAN EDA:")
print("- Dataset awal terdiri dari 2111 baris dan 17 kolom.")
print("- Sebanyak 4 baris duplikat ditemukan dan berhasil dihapus.")
print("- Tidak ditemukan missing values dalam dataset.")
print("- Terdapat ketidakseimbangan pada kelas target 'NObeyesdad', sehingga perlu penanganan (misalnya dengan oversampling atau undersampling.")
print("- Proses deteksi outlier dilakukan pada fitur numerik menggunakan metode IQR (Interquartile Range).")

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

print("Setelah SMOTE:")
print(pd.Series(y_resampled).value_counts())

# Kesimpulan keseluruhan tahap preprocessing
print("\nKESIMPULAN Preprocessing:")
print("- Dataset telah berhasil dibersihkan dan dipersiapkan untuk pemodelan. Data kategorikal diubah menjadi numerik, data numerik dinormalisasi, dan ketidakseimbangan kelas diatasi menggunakan SMOTE. Dataset kini dalam kondisi optimal untuk digunakan dalam algoritma klasifikasi.")

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model yang digunakan
models = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "K-Nearest Neighbors": KNeighborsClassifier()
}

# Evaluasi semua model
results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    results.append({
        'Model': name,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1 Score': f1
    })

        # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix: {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# Tabel hasil evaluasi
results_df = pd.DataFrame(results).set_index('Model')
results_df.plot(kind='bar', figsize=(10, 6), ylim=(0, 1), title='Perbandingan Performa Model')
plt.ylabel("Score")
plt.xticks(rotation=0)
plt.show()

print(results_df)

# Kesimpulan keseluruhan tahap Pemodelan dan Evaluasi
print("\nKESIMPULAN Pemodelan dan Evaluasi:")
print("- Model Random Forest menunjukkan performa paling stabil dan unggul di hampir semua metrik.")
print("- Logistic Regression memiliki performa yang cukup baik namun sedikit menurun dalam menangani kelas yang kompleks.")
print("- KNN memberikan hasil yang lebih rendah dibandingkan dua model lainnya, kemungkinan disebabkan oleh sensitivitas terhadap distribusi data.")

# Parameter grid untuk Random Forest
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'bootstrap': [True, False]
}

# Inisialisasi model
rf = RandomForestClassifier(random_state=42)

# Grid Search
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=1, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Model terbaik
best_rf = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)

# Prediksi dengan model terbaik
y_pred_tuned = best_rf.predict(X_test)

# Evaluasi ulang
acc_tuned = accuracy_score(y_test, y_pred_tuned)
prec_tuned = precision_score(y_test, y_pred_tuned, average='weighted')
rec_tuned = recall_score(y_test, y_pred_tuned, average='weighted')
f1_tuned = f1_score(y_test, y_pred_tuned, average='weighted')

print(f"Akurasi setelah tuning: {acc_tuned:.4f}")
print(f"Presisi : {prec_tuned:.4f}")
print(f"Recall  : {rec_tuned:.4f}")
print(f"F1 Score: {f1_tuned:.4f}")

before = [results_df.loc["Random Forest", "Accuracy"],
          results_df.loc["Random Forest", "Precision"],
          results_df.loc["Random Forest", "Recall"],
          results_df.loc["Random Forest", "F1 Score"]]

after = [acc_tuned, prec_tuned, rec_tuned, f1_tuned]
metrics = ["Accuracy", "Precision", "Recall", "F1 Score"]

x = range(len(metrics))
plt.figure(figsize=(8, 5))
plt.bar(x, before, width=0.4, label="Before", align='center')
plt.bar([p + 0.4 for p in x], after, width=0.4, label="After", align='center')
plt.xticks([p + 0.2 for p in x], metrics)
plt.ylim(0, 1)
plt.title("Perbandingan Performa Random Forest Sebelum dan Sesudah Tuning")
plt.legend()
plt.tight_layout()
plt.show()

# Kesimpulan keseluruhan tahap Hyperparameter Tuning
print("\nKESIMPULAN Hyperparameter Tuning:")
print("- Hyperparameter tuning telah dilakukan menggunakan GridSearchCV pada model Random Forest. Hasil tuning menunjukkan peningkatan performa pada metrik akurasi, presisi, recall, dan F1-score dibandingkan sebelum tuning. Model hasil optimasi ini lebih akurat dan direkomendasikan untuk digunakan pada tahap deployment.")

# Model terbaik
best_rf = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)
joblib.dump(best_rf, "random_forest_model.pkl")


# Load model dan scaler
model = joblib.load('random_forest_model.pkl')
scaler = joblib.load('scaler.pkl')

# Label encoding untuk input pengguna
gender_map = {'Male': 1, 'Female': 0}
family_history_map = {'Yes': 1, 'No': 0}
favc_map = {'Yes': 1, 'No': 0}
caec_map = {'No': 0, 'Sometimes': 1, 'Frequently': 2, 'Always': 3}
smoke_map = {'Yes': 1, 'No': 0}
scc_map = {'Yes': 1, 'No': 0}
calc_map = {'No': 0, 'Sometimes': 1, 'Frequently': 2, 'Always': 3}
mtrans_map = {
    'Automobile': 0,
    'Motorbike': 1,
    'Bike': 2,
    'Public Transportation': 3,
    'Walking': 4
}
label_map = {
    'Insufficient_Weight': 'Berat Badan Kurang',
    'Normal_Weight': 'Berat Badan Normal',
    'Obesity_Type_I': 'Obesitas Tipe I',
    'Obesity_Type_II': 'Obesitas Tipe II',
    'Obesity_Type_III': 'Obesitas Tipe III',
    'Overweight_Level_I': 'Kelebihan Berat Badan Level I',
    'Overweight_Level_II': 'Kelebihan Berat Badan Level II'
}

# Sidebar input pengguna
st.sidebar.header("Input Data Pengguna")
age = st.sidebar.number_input("Umur", min_value=1, max_value=100, value=25)
gender = st.sidebar.selectbox("Jenis Kelamin", list(gender_map.keys()))
height = st.sidebar.number_input("Tinggi Badan (m)", min_value=1.0, max_value=2.5, value=1.70)
weight = st.sidebar.number_input("Berat Badan (kg)", min_value=1.0, max_value=200.0, value=70.0)
calc = st.sidebar.selectbox("Frekuensi konsumsi alkohol", list(calc_map.keys()))
favc = st.sidebar.selectbox("Apakah sering mengonsumsi makanan tinggi kalori?", list(favc_map.keys()))
fcvc = st.sidebar.slider("Frekuensi konsumsi sayur (0: jarang, 3: selalu)", 1.0, 2.0, 3.0)
ncp = st.sidebar.slider("Jumlah makanan utama per hari", 1.0, 4.0, 3.0)
scc = st.sidebar.selectbox("Apakah memonitor kalori makanan?", list(scc_map.keys()))
smoke = st.sidebar.selectbox("Apakah merokok?", list(smoke_map.keys()))
ch2o = st.sidebar.slider("Konsumsi air (liter/hari)", 0.0, 3.0, 2.0)
family_history = st.sidebar.selectbox("Riwayat Kegemukan dalam Keluarga", list(family_history_map.keys()))
faf = st.sidebar.slider("Frekuensi aktivitas fisik (jam/minggu)", 0.0, 5.0, 2.0)
tue = st.sidebar.slider("Waktu menggunakan perangkat elektronik (jam/hari)", 0.0, 5.0, 1.0)
caec = st.sidebar.selectbox("Frekuensi ngemil", list(caec_map.keys()))
mtrans = st.sidebar.selectbox("Transportasi utama", list(mtrans_map.keys()))


# Ubah input ke dalam format numerik
gender_encoded = gender_map[gender]
family_history_encoded = family_history_map[family_history]
favc_encoded = favc_map[favc]
caec_encoded = caec_map[caec]
smoke_encoded = smoke_map[smoke]
scc_encoded = scc_map[scc]
calc_encoded = calc_map[calc]
mtrans_encoded = mtrans_map[mtrans]

# Pisahkan fitur numerik dan kategorikal
numeric_input = np.array([[age, height, weight, fcvc, ncp, ch2o, faf, tue]])
categorical_input = np.array([[gender_encoded, family_history_encoded, favc_encoded, caec_encoded,
                               smoke_encoded, scc_encoded, calc_encoded, mtrans_encoded]])

# Lakukan scaling hanya pada fitur numerik
try:
    numeric_scaled = scaler.transform(numeric_input)
except Exception as e:
    st.error(f"Gagal melakukan scaling: {e}")
    st.stop()

# Gabungkan numerik dan kategorikal menjadi input akhir
final_input = np.concatenate([categorical_input, numeric_scaled], axis=1)

if st.button("Prediksi"):
    try:
        # Lakukan prediksi
        prediction = model.predict(final_input)
        
        # Load kembali LabelEncoder dari file
        encoder = joblib.load("label_encoder.pkl")

        # Ubah hasil prediksi (angka) menjadi label string
        prediction_label = encoder.inverse_transform(prediction)[0]

        # Peta label string ke bahasa Indonesia
        label_map = {
            'Insufficient_Weight': 'Berat Badan Kurang',
            'Normal_Weight': 'Berat Badan Normal',
            'Obesity_Type_I': 'Obesitas Tipe I',
            'Obesity_Type_II': 'Obesitas Tipe II',
            'Obesity_Type_III': 'Obesitas Tipe III',
            'Overweight_Level_I': 'Kelebihan Berat Badan Level I',
            'Overweight_Level_II': 'Kelebihan Berat Badan Level II'
        }

        # Ambil label bahasa Indonesia atau fallback ke label asli
        label = label_map.get(prediction_label, prediction_label)

        # Tampilkan hasil prediksi
        st.success(f"Hasil Prediksi: {label}")

    except Exception as e:
        st.error(f"Terjadi kesalahan saat prediksi: {e}")


