# -*- coding: utf-8 -*-
"""PROJECT_UAS_BENGKOD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qvuxgGLBfmpSD8XMlTHHL2rRHFjJS4gl
"""

#!pip install streamlit

import streamlit as st
import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#pip install imblearn



from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

#import joblib
import warnings
warnings.filterwarnings('ignore')

file_path = 'ObesityDataSet.csv'
df = pd.read_csv(file_path)

head = df.head()
info = df.info()
description = df.describe(include='all')

head, description

print(df.info())

# Konversi kolom numerik ke float
numeric_columns = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']
for col in numeric_columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')  # ubah ke float, NaN jika gagal

# Visualisasi distribusi label
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='NObeyesdad', order=df['NObeyesdad'].value_counts().index)
plt.xticks(rotation=45)
plt.title('Distribusi Kelas NObeyesdad')
plt.tight_layout()
plt.show()

# Cek missing values
missing_values = df.isnull().sum()
print(df.isnull().sum())

# Cek data duplikat
duplicates = df.duplicated().sum()
print(df.duplicated().sum())

# Cek nilai unik tiap kolom
unique_values = df.nunique()
print(df.nunique())

# Boxplot untuk mendeteksi outlier pada kolom numerik
plt.figure(figsize=(14, 10))
for i, col in enumerate(numeric_columns, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(x=df[col])
    plt.title(f'Boxplot of {col}')
plt.tight_layout()
plt.show()

missing_values, duplicates, unique_values

# Salin data asli
df_clean = df.copy()

print("Missing values:\n", df_clean.isnull().sum())

# Hapus data duplikat
df_clean = df_clean.drop_duplicates()

# Kategorikal: imputasi dengan modus
cat_columns = df_clean.select_dtypes(include='object').columns.drop('NObeyesdad')
cat_imputer = SimpleImputer(strategy='most_frequent')
df_clean[cat_columns] = cat_imputer.fit_transform(df_clean[cat_columns])

print("Menghapus outlier menggunakan metode IQR...")

outlier_counts = {}

for col in numeric_columns:
    Q1 = df_clean[col].quantile(0.25)
    Q3 = df_clean[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    before_rows = df_clean.shape[0]
    df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]
    after_rows = df_clean.shape[0]
    outlier_counts[col] = before_rows - after_rows

# Tampilkan hasil penghapusan per kolom
for col, count in outlier_counts.items():
    print(f"- {col}: {count} baris dihapus")

#Tentukan kolom kategorikal dan numerik
cat_columns = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']
numeric_columns = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']

#Encoding kategorikal
from sklearn.preprocessing import LabelEncoder, StandardScaler
encoder = LabelEncoder()
for col in cat_columns:
    df_clean[col] = encoder.fit_transform(df_clean[col])

# Encode target (wajib setelah encode fitur)
df_clean['NObeyesdad'] = encoder.fit_transform(df_clean['NObeyesdad'])

# Normalisasi/standarisasi data numerik
scaler = StandardScaler()
df_clean[numeric_columns] = scaler.fit_transform(df_clean[numeric_columns])

# Save the scaler
joblib.dump(scaler, "scaler.pkl")

# Pisahkan fitur dan target
X = df_clean.drop('NObeyesdad', axis=1)
y = df_clean['NObeyesdad']

# Tampilkan distribusi kelas
print("Distribusi sebelum SMOTE:")
print(y.value_counts())

# Hitung korelasi numerik
corr_matrix = df.corr(numeric_only=True)
plt.figure(figsize=(10, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

# Cek ulang info dataset setelah preprocessing
df_clean.info()



for col in numeric_columns:
    plt.figure(figsize=(5, 3))
    sns.boxplot(x=df_clean[col])
    plt.title(f'Boxplot {col} (Setelah Outlier Dihapus)')
    plt.tight_layout()
    plt.show()

# Kesimpulan keseluruhan tahap EDA
print("\nKESIMPULAN EDA:")
print("- Dataset awal terdiri dari 2111 baris dan 17 kolom.")
print("- Sebanyak 4 baris duplikat ditemukan dan berhasil dihapus.")
print("- Tidak ditemukan missing values dalam dataset.")
print("- Terdapat ketidakseimbangan pada kelas target 'NObeyesdad', sehingga perlu penanganan (misalnya dengan oversampling atau undersampling.")
print("- Proses deteksi outlier dilakukan pada fitur numerik menggunakan metode IQR (Interquartile Range).")

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

print("Setelah SMOTE:")
print(pd.Series(y_resampled).value_counts())

# Kesimpulan keseluruhan tahap preprocessing
print("\nKESIMPULAN Preprocessing:")
print("- Dataset telah berhasil dibersihkan dan dipersiapkan untuk pemodelan. Data kategorikal diubah menjadi numerik, data numerik dinormalisasi, dan ketidakseimbangan kelas diatasi menggunakan SMOTE. Dataset kini dalam kondisi optimal untuk digunakan dalam algoritma klasifikasi.")

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model yang digunakan
models = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "K-Nearest Neighbors": KNeighborsClassifier()
}

# Evaluasi semua model
results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    results.append({
        'Model': name,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1 Score': f1
    })

        # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix: {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# Tabel hasil evaluasi
results_df = pd.DataFrame(results).set_index('Model')
results_df.plot(kind='bar', figsize=(10, 6), ylim=(0, 1), title='Perbandingan Performa Model')
plt.ylabel("Score")
plt.xticks(rotation=0)
plt.show()

print(results_df)

# Kesimpulan keseluruhan tahap Pemodelan dan Evaluasi
print("\nKESIMPULAN Pemodelan dan Evaluasi:")
print("- Model Random Forest menunjukkan performa paling stabil dan unggul di hampir semua metrik.")
print("- Logistic Regression memiliki performa yang cukup baik namun sedikit menurun dalam menangani kelas yang kompleks.")
print("- KNN memberikan hasil yang lebih rendah dibandingkan dua model lainnya, kemungkinan disebabkan oleh sensitivitas terhadap distribusi data.")

# Parameter grid untuk Random Forest
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'bootstrap': [True, False]
}

# Inisialisasi model
rf = RandomForestClassifier(random_state=42)

# Grid Search
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=1, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Model terbaik
best_rf = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)

# Prediksi dengan model terbaik
y_pred_tuned = best_rf.predict(X_test)

# Evaluasi ulang
acc_tuned = accuracy_score(y_test, y_pred_tuned)
prec_tuned = precision_score(y_test, y_pred_tuned, average='weighted')
rec_tuned = recall_score(y_test, y_pred_tuned, average='weighted')
f1_tuned = f1_score(y_test, y_pred_tuned, average='weighted')

print(f"Akurasi setelah tuning: {acc_tuned:.4f}")
print(f"Presisi : {prec_tuned:.4f}")
print(f"Recall  : {rec_tuned:.4f}")
print(f"F1 Score: {f1_tuned:.4f}")

before = [results_df.loc["Random Forest", "Accuracy"],
          results_df.loc["Random Forest", "Precision"],
          results_df.loc["Random Forest", "Recall"],
          results_df.loc["Random Forest", "F1 Score"]]

after = [acc_tuned, prec_tuned, rec_tuned, f1_tuned]
metrics = ["Accuracy", "Precision", "Recall", "F1 Score"]

x = range(len(metrics))
plt.figure(figsize=(8, 5))
plt.bar(x, before, width=0.4, label="Before", align='center')
plt.bar([p + 0.4 for p in x], after, width=0.4, label="After", align='center')
plt.xticks([p + 0.2 for p in x], metrics)
plt.ylim(0, 1)
plt.title("Perbandingan Performa Random Forest Sebelum dan Sesudah Tuning")
plt.legend()
plt.tight_layout()
plt.show()

# Kesimpulan keseluruhan tahap Hyperparameter Tuning
print("\nKESIMPULAN Hyperparameter Tuning:")
print("- Hyperparameter tuning telah dilakukan menggunakan GridSearchCV pada model Random Forest. Hasil tuning menunjukkan peningkatan performa pada metrik akurasi, presisi, recall, dan F1-score dibandingkan sebelum tuning. Model hasil optimasi ini lebih akurat dan direkomendasikan untuk digunakan pada tahap deployment.")

# Model terbaik
best_rf = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)

# Save the best model
model = joblib.load("random_forest_model.pkl")
scaler = joblib.load("scaler.pkl")  # Simpan dan load scaler yang digunakan saat training

# Prediksi dengan model terbaik
y_pred_tuned = best_rf.predict(X_test)

st.title("Prediksi Tingkat Obesitas")
st.write("Isi data berikut untuk memprediksi status berat badan Anda:")

# Input user
age = st.slider("Usia", 10, 100)
height = st.slider("Tinggi Badan (meter)", 1.0, 2.5, step=0.01)
weight = st.slider("Berat Badan (kg)", 20.0, 200.0, step=0.5)
fcvc = st.slider("Konsumsi Sayur (1 - jarang, 3 - sering)", 1, 3)
ncp = st.slider("Jumlah makan besar per hari", 1, 4)
ch2o = st.slider("Konsumsi air harian (1 - sedikit, 3 - banyak)", 1, 3)
faf = st.slider("Frekuensi aktivitas fisik (0 - tidak pernah, 3 - rutin)", 0, 3)
tue = st.slider("Waktu screen time (jam/hari)", 0, 3)

# Gabungkan ke dalam array
input_data = np.array([[age, height, weight, fcvc, ncp, ch2o, faf, tue]])

# Standarisasi input
input_scaled = scaler.transform(input_data)

# Mapping hasil prediksi ke label
label_map = {
    0: "Insufficient_Weight", 
    1: "Normal_Weight", 
    2: "Overweight_Level_I",
    3: "Overweight_Level_II", 
    4: "Obesity_Type_I",
    5: "Obesity_Type_II", 
    6: "Obesity_Type_III"
}

if st.button("Prediksi"):
    prediction = model.predict(input_scaled)
    label = label_map.get(prediction[0], "Unknown")
    st.success(f"Hasil Prediksi: {label}")
